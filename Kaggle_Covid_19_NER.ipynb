{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Kaggle1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHWUwb3LUyVdxLYT0WiFlT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckqqqq/Kaggle_Covid_19_NER/blob/main/Kaggle_Covid_19_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "Znizf_ugcBjz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDhudYokjOkT"
      },
      "source": [
        "# !pip3 install torch==1.8.2+cu102 torchvision==0.9.2+cu102 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJziwv608zG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55d5718-c6eb-4df2-c979-55ef9e73875c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4gGDTz79Axy",
        "outputId": "71d29208-089b-4701-e40f-d9b2592c9fab"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/Kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdmfWLgGobRa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FRbPiva-dq3"
      },
      "source": [
        "#!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do_SYelI9G1m"
      },
      "source": [
        "# Auchor Kaggle \n",
        "# Keqi Chen \n",
        "# tianqingfang https://www.kaggle.com/tianqingfang/playground Standard inputoutput\n",
        "# Pytorch Document :BILSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle as pkl\n",
        "from itertools import chain\n",
        "import pandas as pd\n",
        "#import pycuda#\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "#core\n",
        "torch.manual_seed(1)\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 32\n",
        "EPOCH = 1\n",
        "gpu_use=torch.cuda.is_available()\n",
        "def argmax(vec):\n",
        "    # return the argmax \n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        " # Transferm seq_lest to Seq_tesor\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long) \n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "\n",
        "# main model\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "    # vocabulary size| mag_(label-number)| embedding dimension| hidden dimension\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.gpu_use=torch.cuda.is_available()\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "        \n",
        "\n",
        "        # \"//\"\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # transition parameters  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        # initialization\n",
        "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        # initial the Start and Tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "        if self.gpu_use:#accerlerate\n",
        "          #print(\"GPU\")\n",
        "          self.lstm.cuda()\n",
        "          self.hidden2tag.cuda()\n",
        "          self.transitions.cuda()\n",
        "        \n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the *forward algorithm* to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        if self.gpu_use:#accerlerate\n",
        "          #print(\"GPU)\n",
        "          init_alphas.cuda()\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "            if self.gpu_use:#accerlerate\n",
        "              #print(\"GPU)\n",
        "              forward_var.cuda()\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    \n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                    self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        #\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "    #dijkstrbi\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  \n",
        "            # holds the backpointers for this step\n",
        "            viterbivars_t = []  \n",
        "            # holds the viterbi variables for this step\n",
        "            # https://www.zhihu.com/question/20136144\n",
        "            # 233333 Like dijkstra algorithm\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        # predict\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check ??\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  \n",
        "        # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features. \n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        # Explanation https://space.bilibili.com/88461692/channel/detail?cid=26587\n",
        "        return score, tag_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFeekNtP9x4b",
        "outputId": "02d320ec-6ea4-4306-b699-2fd8be36c36b"
      },
      "source": [
        "#\n",
        "path =r\"/content/drive/MyDrive/Colab Notebooks/Kaggle/\"\n",
        "train_dict = pkl.load(open(path+r\"/Kaggle_input/train.pkl\", \"rb\"))\n",
        "val_dict = pkl.load(open(path+r\"/Kaggle_input/val.pkl\", \"rb\"))\n",
        "test_dict = pkl.load(open(path+r\"/Kaggle_input/test.pkl\", \"rb\"))\n",
        "print(\"keys in train_dict:\", train_dict.keys())\n",
        "print(\"keys in val_dict:\", val_dict.keys())\n",
        "print(\"keys in test_dict:\", test_dict.keys())\n",
        "print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
        "# print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))\n",
        "# prepare word vocab and tag vocab\n",
        "\n",
        "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
        "# vocab\n",
        "for doc in train_dict['word_seq']:\n",
        "    for word in doc:\n",
        "        if (word not in vocab_dict):\n",
        "            vocab_dict[word] = len(vocab_dict)\n",
        "# attention [0]\n",
        "tag_dict = {'_t_pad_': 0}  \n",
        "# add a padding token\n",
        "# tags\n",
        "for tag_seq in train_dict['tag_seq']:\n",
        "    for tag in tag_seq:\n",
        "        if (tag not in tag_dict):\n",
        "            tag_dict[tag] = len(tag_dict)\n",
        "word2idx = vocab_dict\n",
        "idx2word = {v: k \\\n",
        "             for k, v in word2idx.items()}# reverse！！！review Note\n",
        "# print(idx2word)# too many\n",
        "tag2idx = tag_dict\n",
        "idx2tag = {v: k \\\n",
        "            for k, v in tag2idx.items()} #reverse！！！1111111\n",
        "tag2idx[START_TAG] = len(tag2idx)\n",
        "tag2idx[STOP_TAG] = len(tag2idx)\n",
        "print(tag2idx)\n",
        "\n",
        "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))\n",
        "print(\"Model initialized\")\n",
        "# model = BiLSTM_CRF(len(word2idx), tag2idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "\n",
        "# The maximum length of a sentence is set to 128\n",
        "max_sent_length = 128\n",
        "\n",
        "train_tokens = np.array([[word2idx[w] \\\n",
        "                          for w in doc] \\\n",
        "                         for doc in train_dict['word_seq']])\n",
        "# for test tp prevent overfit\n",
        "val_tokens = np.array([[word2idx.get(w, 0)  \\\n",
        "                        for w in doc] \\\n",
        "                       for doc in val_dict['word_seq']])\n",
        "# Useless in this question\n",
        "\n",
        "\n",
        "\n",
        "train_tags = [[tag2idx[t] \\\n",
        "               for t in t_seq] \\\n",
        "              for t_seq in train_dict['tag_seq']]\n",
        "#transfrom to binary file\n",
        "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) \\\n",
        "                       for t_seq in train_tags])\n",
        "\n",
        "val_tags = [[tag2idx[t] \\\n",
        "             for t in t_seq] \\\n",
        "            for t_seq in val_dict['tag_seq']]\n",
        "\n",
        "# val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) \\\n",
        "#                      for t_seq in val_tags])\n",
        "\n",
        "# we don't have test tags\n",
        "print(val_tags[0])\n",
        "print(train_tags[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
            "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
            "keys in test_dict: dict_keys(['id', 'word_seq'])\n",
            "count of the NER tags: 65\n",
            "{'_t_pad_': 0, 'O': 1, 'LIVESTOCK': 2, 'DISEASE_OR_SYNDROME': 3, 'GENE_OR_GENOME': 4, 'CARDINAL': 5, 'CHEMICAL': 6, 'PRODUCT': 7, 'QUANTITY': 8, 'NORP': 9, 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE': 10, 'CELL': 11, 'ORGANISM': 12, 'GROUP': 13, 'ORDINAL': 14, 'GPE': 15, 'ORG': 16, 'LABORATORY_PROCEDURE': 17, 'DATE': 18, 'CORONAVIRUS': 19, 'EUKARYOTE': 20, 'SIGN_OR_SYMPTOM': 21, 'VIRUS': 22, 'CELL_COMPONENT': 23, 'MOLECULAR_FUNCTION': 24, 'CELL_OR_MOLECULAR_DYSFUNCTION': 25, 'VIRAL_PROTEIN': 26, 'HUMAN-CAUSED_PHENOMENON_OR_PROCESS': 27, 'BODY_PART_ORGAN_OR_ORGAN_COMPONENT': 28, 'PERSON': 29, 'TISSUE': 30, 'RESEARCH_ACTIVITY': 31, 'EVENT': 32, 'IMMUNE_RESPONSE': 33, 'ORGAN_OR_TISSUE_FUNCTION': 34, 'MATERIAL': 35, 'EVOLUTION': 36, 'LABORATORY_OR_TEST_RESULT': 37, 'BACTERIUM': 38, 'MONEY': 39, 'FAC': 40, 'DAILY_OR_RECREATIONAL_ACTIVITY': 41, 'ANATOMICAL_STRUCTURE': 42, 'CELL_FUNCTION': 43, 'SUBSTRATE': 44, 'INDIVIDUAL_BEHAVIOR': 45, 'BODY_SUBSTANCE': 46, 'SOCIAL_BEHAVIOR': 47, 'WILDLIFE': 48, 'LOC': 49, 'LAW': 50, 'INJURY_OR_POISONING': 51, 'DIAGNOSTIC_PROCEDURE': 52, 'TIME': 53, 'EXPERIMENTAL_MODEL_OF_DISEASE': 54, 'GOVERNMENTAL_OR_REGULATORY_ACTIVITY': 55, 'PERCENT': 56, 'FOOD': 57, 'WORK_OF_ART': 58, 'MACHINE_ACTIVITY': 59, 'LANGUAGE': 60, 'EDUCATIONAL_ACTIVITY': 61, 'GROUP_ATTRIBUTE': 62, 'PHYSICAL_SCIENCE': 63, 'ARCHAEON': 64, '<START>': 65, '<STOP>': 66}\n",
            "size of word vocab: 82275 size of tag_dict: 67\n",
            "Model initialized\n",
            "[1, 1, 1, 6, 6, 1, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 33, 1, 1, 1, 6, 1, 1, 6, 6, 6, 6, 1, 1, 1, 6, 6, 1, 1, 1, 1, 5, 6, 6, 1, 1, 33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 20, 1, 20, 1, 1, 6, 6, 1, 5, 5, 1, 1, 1, 1, 1, 1, 13, 1, 1, 6, 33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 1]\n",
            "[1, 1, 2, 1, 1, 3, 3, 1, 4, 4, 4, 4, 4, 1, 5, 5, 5, 1, 1, 6, 6, 1, 4, 1, 1, 6, 6, 6, 1, 1, 1, 4, 4, 4, 1, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 1, 1, 1, 1, 8, 8, 1, 1, 4, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 6, 6, 1, 1, 6, 6, 6, 1, 1, 1, 6, 6, 1, 1, 1, 1, 1, 6, 1, 1, 6, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZVjgbROqFz1"
      },
      "source": [
        "test_tokens = np.array([[word2idx.get(w, 0) \\\n",
        "                         for w in doc] \\\n",
        "                        for doc in test_dict['word_seq']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTNDe4PY7FU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0bd934-ec00-4e70-ac06-bdd26680f6d5"
      },
      "source": [
        "\n",
        "\n",
        "#accerlerate\n",
        "if gpu_use :\n",
        "  # pycuda.cuda.init()\n",
        "  \n",
        "  # torch.load('modelparameters.pth', map_location=lambda storage, loc: storage.cuda(0))\n",
        "\n",
        "  print(\"Cuda:Version\",torch.version.cuda)\n",
        "  print(torch.cuda.current_device())\n",
        "  # num = cuda.Device.count()\n",
        "  # print(\"%d device(s) found:\"%num)\n",
        "  # for i in range(num):\n",
        "  #   print(cuda.Device(i).name(), \"(Id: %d)\"%i)\n",
        "else:\n",
        "  print(\"Warnning No Cuda ！setting_ GPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warnning No Cuda ！setting_ GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnImuU6SXJ7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb13da1-56f0-4bee-a46e-1760a6716ba8"
      },
      "source": [
        "print(\"model initailization:\\n\")\n",
        "model = BiLSTM_CRF(len(word2idx), tag2idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "if gpu_use :#accerlerate \n",
        "  model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model initailization:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRcS_2-H-RiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15161238-f57d-4577-8457-7090e8478498"
      },
      "source": [
        "\n",
        "\n",
        "#https://blog.csdn.net/xiaosongshine/article/details/89401522\n",
        "#https://blog.csdn.net/w113691/article/details/106571007/\n",
        "   \n",
        "\n",
        "# optimize\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, weight_decay=1e-4)\n",
        "# We are short on GPU\n",
        "\n",
        "#Check predictions before training\n",
        "# with torch.no_grad():\n",
        "#     precheck_sent = prepare_sequence(val_tokens, word2idx)\n",
        "#     precheck_tags = torch.tensor([tag2idx[t] for t in val_tags], dtype=torch.long)\n",
        "#     print(model(precheck_sent))\n",
        "print(\"Training beginning\\n\")\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "EPOCH=5\n",
        "#28000\n",
        "#\n",
        "for ep in range(EPOCH):  # \n",
        "    Counter =0\n",
        "    for sentence_sqe, tags_sqe in zip(val_tokens,val_tags):\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        # sentence_in = prepare_sequence(sentence, word2idx)\n",
        "\n",
        "        sentence_tor = torch.tensor(sentence_sqe, dtype=torch.long)\n",
        "        tag_tor = torch.tensor(tags_sqe, dtype=torch.long)\n",
        "        if gpu_use:#accerlerate\n",
        "          #print(\"GPU\")\n",
        "          sentence_tor.cuda()\n",
        "          tag_tor.cuda()\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_tor, tag_tor)\n",
        "      \n",
        "        \n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        Counter += 1\n",
        "        if Counter%10==0:\n",
        "          print(Counter,\"Loss: \",loss)\n",
        "        if Counter==400:\n",
        "          break\n",
        "\n",
        "        #128*128\n",
        "        #128*128 2800 val 28000 train *100000\n",
        "        \n",
        "print(\"Training finished!!!!!:\")\n",
        "\n",
        "\n",
        "# We got it!\n",
        "#Check predictions after training\n",
        "predict=[]\n",
        "Counter=0\n",
        "with torch.no_grad():\n",
        "  print(model(torch.tensor(val_tokens[1],dtype=torch.long)))\n",
        "with torch.no_grad():\n",
        "  print(torch.tensor(val_tokens,dtype=torch.long))\n",
        "  for val_token in val_tokens:\n",
        "    predict.append(model(torch.tensor(val_token,dtype=torch.long))[1])  \n",
        "    \n",
        "\n",
        "print(\"Predic type:\", type(predict))\n",
        "\n",
        "val_preds = np.array([[idx2tag[p] for p in pred_tor] for pred_tor in predict])\n",
        "print(val_preds[2])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training beginning\n",
            "\n",
            "10 Loss:  tensor([195.9117], grad_fn=<SubBackward0>)\n",
            "20 Loss:  tensor([450.0818], grad_fn=<SubBackward0>)\n",
            "30 Loss:  tensor([218.0327], grad_fn=<SubBackward0>)\n",
            "40 Loss:  tensor([166.6262], grad_fn=<SubBackward0>)\n",
            "50 Loss:  tensor([150.3998], grad_fn=<SubBackward0>)\n",
            "60 Loss:  tensor([162.1233], grad_fn=<SubBackward0>)\n",
            "70 Loss:  tensor([93.0287], grad_fn=<SubBackward0>)\n",
            "80 Loss:  tensor([185.0294], grad_fn=<SubBackward0>)\n",
            "90 Loss:  tensor([171.3558], grad_fn=<SubBackward0>)\n",
            "100 Loss:  tensor([139.9319], grad_fn=<SubBackward0>)\n",
            "110 Loss:  tensor([110.7762], grad_fn=<SubBackward0>)\n",
            "120 Loss:  tensor([147.6701], grad_fn=<SubBackward0>)\n",
            "130 Loss:  tensor([136.1213], grad_fn=<SubBackward0>)\n",
            "140 Loss:  tensor([50.1854], grad_fn=<SubBackward0>)\n",
            "150 Loss:  tensor([146.6354], grad_fn=<SubBackward0>)\n",
            "160 Loss:  tensor([138.0318], grad_fn=<SubBackward0>)\n",
            "170 Loss:  tensor([36.8644], grad_fn=<SubBackward0>)\n",
            "180 Loss:  tensor([168.7064], grad_fn=<SubBackward0>)\n",
            "190 Loss:  tensor([29.3525], grad_fn=<SubBackward0>)\n",
            "200 Loss:  tensor([149.4748], grad_fn=<SubBackward0>)\n",
            "210 Loss:  tensor([134.1544], grad_fn=<SubBackward0>)\n",
            "220 Loss:  tensor([198.6161], grad_fn=<SubBackward0>)\n",
            "230 Loss:  tensor([147.1229], grad_fn=<SubBackward0>)\n",
            "240 Loss:  tensor([180.5883], grad_fn=<SubBackward0>)\n",
            "250 Loss:  tensor([167.7149], grad_fn=<SubBackward0>)\n",
            "260 Loss:  tensor([10.8939], grad_fn=<SubBackward0>)\n",
            "270 Loss:  tensor([89.9520], grad_fn=<SubBackward0>)\n",
            "280 Loss:  tensor([85.9858], grad_fn=<SubBackward0>)\n",
            "290 Loss:  tensor([97.4019], grad_fn=<SubBackward0>)\n",
            "300 Loss:  tensor([162.1081], grad_fn=<SubBackward0>)\n",
            "310 Loss:  tensor([170.4678], grad_fn=<SubBackward0>)\n",
            "320 Loss:  tensor([165.8887], grad_fn=<SubBackward0>)\n",
            "330 Loss:  tensor([9.1434], grad_fn=<SubBackward0>)\n",
            "340 Loss:  tensor([110.6254], grad_fn=<SubBackward0>)\n",
            "350 Loss:  tensor([112.3597], grad_fn=<SubBackward0>)\n",
            "360 Loss:  tensor([167.3727], grad_fn=<SubBackward0>)\n",
            "370 Loss:  tensor([143.3739], grad_fn=<SubBackward0>)\n",
            "380 Loss:  tensor([144.8682], grad_fn=<SubBackward0>)\n",
            "390 Loss:  tensor([131.6927], grad_fn=<SubBackward0>)\n",
            "400 Loss:  tensor([8.0978], grad_fn=<SubBackward0>)\n",
            "10 Loss:  tensor([135.0678], grad_fn=<SubBackward0>)\n",
            "20 Loss:  tensor([39.6935], grad_fn=<SubBackward0>)\n",
            "30 Loss:  tensor([173.8809], grad_fn=<SubBackward0>)\n",
            "40 Loss:  tensor([111.3339], grad_fn=<SubBackward0>)\n",
            "50 Loss:  tensor([111.6451], grad_fn=<SubBackward0>)\n",
            "60 Loss:  tensor([104.2436], grad_fn=<SubBackward0>)\n",
            "70 Loss:  tensor([83.7391], grad_fn=<SubBackward0>)\n",
            "80 Loss:  tensor([141.2869], grad_fn=<SubBackward0>)\n",
            "90 Loss:  tensor([142.1021], grad_fn=<SubBackward0>)\n",
            "100 Loss:  tensor([97.2260], grad_fn=<SubBackward0>)\n",
            "110 Loss:  tensor([83.7805], grad_fn=<SubBackward0>)\n",
            "120 Loss:  tensor([116.6144], grad_fn=<SubBackward0>)\n",
            "130 Loss:  tensor([120.2283], grad_fn=<SubBackward0>)\n",
            "140 Loss:  tensor([60.1089], grad_fn=<SubBackward0>)\n",
            "150 Loss:  tensor([80.7772], grad_fn=<SubBackward0>)\n",
            "160 Loss:  tensor([107.5084], grad_fn=<SubBackward0>)\n",
            "170 Loss:  tensor([16.9607], grad_fn=<SubBackward0>)\n",
            "180 Loss:  tensor([134.5054], grad_fn=<SubBackward0>)\n",
            "190 Loss:  tensor([22.6920], grad_fn=<SubBackward0>)\n",
            "200 Loss:  tensor([112.1064], grad_fn=<SubBackward0>)\n",
            "210 Loss:  tensor([126.3732], grad_fn=<SubBackward0>)\n",
            "220 Loss:  tensor([177.4432], grad_fn=<SubBackward0>)\n",
            "230 Loss:  tensor([125.1193], grad_fn=<SubBackward0>)\n",
            "240 Loss:  tensor([130.8169], grad_fn=<SubBackward0>)\n",
            "250 Loss:  tensor([147.5180], grad_fn=<SubBackward0>)\n",
            "260 Loss:  tensor([10.4858], grad_fn=<SubBackward0>)\n",
            "270 Loss:  tensor([79.6312], grad_fn=<SubBackward0>)\n",
            "280 Loss:  tensor([76.1296], grad_fn=<SubBackward0>)\n",
            "290 Loss:  tensor([79.8268], grad_fn=<SubBackward0>)\n",
            "300 Loss:  tensor([131.0775], grad_fn=<SubBackward0>)\n",
            "310 Loss:  tensor([157.9039], grad_fn=<SubBackward0>)\n",
            "320 Loss:  tensor([136.9785], grad_fn=<SubBackward0>)\n",
            "330 Loss:  tensor([7.5574], grad_fn=<SubBackward0>)\n",
            "340 Loss:  tensor([97.7769], grad_fn=<SubBackward0>)\n",
            "350 Loss:  tensor([85.6462], grad_fn=<SubBackward0>)\n",
            "360 Loss:  tensor([145.9774], grad_fn=<SubBackward0>)\n",
            "370 Loss:  tensor([120.8103], grad_fn=<SubBackward0>)\n",
            "380 Loss:  tensor([133.5632], grad_fn=<SubBackward0>)\n",
            "390 Loss:  tensor([114.9771], grad_fn=<SubBackward0>)\n",
            "400 Loss:  tensor([6.4019], grad_fn=<SubBackward0>)\n",
            "10 Loss:  tensor([118.8845], grad_fn=<SubBackward0>)\n",
            "20 Loss:  tensor([35.7802], grad_fn=<SubBackward0>)\n",
            "30 Loss:  tensor([154.1718], grad_fn=<SubBackward0>)\n",
            "40 Loss:  tensor([98.9450], grad_fn=<SubBackward0>)\n",
            "50 Loss:  tensor([94.9399], grad_fn=<SubBackward0>)\n",
            "60 Loss:  tensor([88.5096], grad_fn=<SubBackward0>)\n",
            "70 Loss:  tensor([74.3838], grad_fn=<SubBackward0>)\n",
            "80 Loss:  tensor([119.9111], grad_fn=<SubBackward0>)\n",
            "90 Loss:  tensor([121.7654], grad_fn=<SubBackward0>)\n",
            "100 Loss:  tensor([88.4267], grad_fn=<SubBackward0>)\n",
            "110 Loss:  tensor([72.3182], grad_fn=<SubBackward0>)\n",
            "120 Loss:  tensor([108.2701], grad_fn=<SubBackward0>)\n",
            "130 Loss:  tensor([106.9429], grad_fn=<SubBackward0>)\n",
            "140 Loss:  tensor([58.8369], grad_fn=<SubBackward0>)\n",
            "150 Loss:  tensor([56.7189], grad_fn=<SubBackward0>)\n",
            "160 Loss:  tensor([95.4244], grad_fn=<SubBackward0>)\n",
            "170 Loss:  tensor([12.4045], grad_fn=<SubBackward0>)\n",
            "180 Loss:  tensor([120.0562], grad_fn=<SubBackward0>)\n",
            "190 Loss:  tensor([18.3296], grad_fn=<SubBackward0>)\n",
            "200 Loss:  tensor([99.0390], grad_fn=<SubBackward0>)\n",
            "210 Loss:  tensor([121.4130], grad_fn=<SubBackward0>)\n",
            "220 Loss:  tensor([145.9982], grad_fn=<SubBackward0>)\n",
            "230 Loss:  tensor([112.3959], grad_fn=<SubBackward0>)\n",
            "240 Loss:  tensor([113.3230], grad_fn=<SubBackward0>)\n",
            "250 Loss:  tensor([125.2020], grad_fn=<SubBackward0>)\n",
            "260 Loss:  tensor([11.7579], grad_fn=<SubBackward0>)\n",
            "270 Loss:  tensor([71.4343], grad_fn=<SubBackward0>)\n",
            "280 Loss:  tensor([68.7222], grad_fn=<SubBackward0>)\n",
            "290 Loss:  tensor([63.4662], grad_fn=<SubBackward0>)\n",
            "300 Loss:  tensor([118.6196], grad_fn=<SubBackward0>)\n",
            "310 Loss:  tensor([149.8860], grad_fn=<SubBackward0>)\n",
            "320 Loss:  tensor([118.3885], grad_fn=<SubBackward0>)\n",
            "330 Loss:  tensor([6.3943], grad_fn=<SubBackward0>)\n",
            "340 Loss:  tensor([90.0191], grad_fn=<SubBackward0>)\n",
            "350 Loss:  tensor([72.1403], grad_fn=<SubBackward0>)\n",
            "360 Loss:  tensor([139.3657], grad_fn=<SubBackward0>)\n",
            "370 Loss:  tensor([109.5978], grad_fn=<SubBackward0>)\n",
            "380 Loss:  tensor([124.1162], grad_fn=<SubBackward0>)\n",
            "390 Loss:  tensor([108.9427], grad_fn=<SubBackward0>)\n",
            "400 Loss:  tensor([6.8920], grad_fn=<SubBackward0>)\n",
            "10 Loss:  tensor([108.8167], grad_fn=<SubBackward0>)\n",
            "20 Loss:  tensor([31.6412], grad_fn=<SubBackward0>)\n",
            "30 Loss:  tensor([138.2843], grad_fn=<SubBackward0>)\n",
            "40 Loss:  tensor([94.2179], grad_fn=<SubBackward0>)\n",
            "50 Loss:  tensor([81.7581], grad_fn=<SubBackward0>)\n",
            "60 Loss:  tensor([80.6635], grad_fn=<SubBackward0>)\n",
            "70 Loss:  tensor([66.5134], grad_fn=<SubBackward0>)\n",
            "80 Loss:  tensor([109.4702], grad_fn=<SubBackward0>)\n",
            "90 Loss:  tensor([108.8273], grad_fn=<SubBackward0>)\n",
            "100 Loss:  tensor([76.1517], grad_fn=<SubBackward0>)\n",
            "110 Loss:  tensor([66.6940], grad_fn=<SubBackward0>)\n",
            "120 Loss:  tensor([100.3130], grad_fn=<SubBackward0>)\n",
            "130 Loss:  tensor([95.2939], grad_fn=<SubBackward0>)\n",
            "140 Loss:  tensor([57.3376], grad_fn=<SubBackward0>)\n",
            "150 Loss:  tensor([43.2443], grad_fn=<SubBackward0>)\n",
            "160 Loss:  tensor([87.8769], grad_fn=<SubBackward0>)\n",
            "170 Loss:  tensor([10.7729], grad_fn=<SubBackward0>)\n",
            "180 Loss:  tensor([110.3349], grad_fn=<SubBackward0>)\n",
            "190 Loss:  tensor([17.4786], grad_fn=<SubBackward0>)\n",
            "200 Loss:  tensor([88.3471], grad_fn=<SubBackward0>)\n",
            "210 Loss:  tensor([110.9029], grad_fn=<SubBackward0>)\n",
            "220 Loss:  tensor([126.9216], grad_fn=<SubBackward0>)\n",
            "230 Loss:  tensor([106.1806], grad_fn=<SubBackward0>)\n",
            "240 Loss:  tensor([100.2767], grad_fn=<SubBackward0>)\n",
            "250 Loss:  tensor([103.3687], grad_fn=<SubBackward0>)\n",
            "260 Loss:  tensor([10.8765], grad_fn=<SubBackward0>)\n",
            "270 Loss:  tensor([68.1259], grad_fn=<SubBackward0>)\n",
            "280 Loss:  tensor([59.5111], grad_fn=<SubBackward0>)\n",
            "290 Loss:  tensor([47.2332], grad_fn=<SubBackward0>)\n",
            "300 Loss:  tensor([107.5697], grad_fn=<SubBackward0>)\n",
            "310 Loss:  tensor([142.2897], grad_fn=<SubBackward0>)\n",
            "320 Loss:  tensor([102.6578], grad_fn=<SubBackward0>)\n",
            "330 Loss:  tensor([5.4823], grad_fn=<SubBackward0>)\n",
            "340 Loss:  tensor([83.7422], grad_fn=<SubBackward0>)\n",
            "350 Loss:  tensor([68.7001], grad_fn=<SubBackward0>)\n",
            "360 Loss:  tensor([131.9136], grad_fn=<SubBackward0>)\n",
            "370 Loss:  tensor([100.8823], grad_fn=<SubBackward0>)\n",
            "380 Loss:  tensor([117.2722], grad_fn=<SubBackward0>)\n",
            "390 Loss:  tensor([99.8995], grad_fn=<SubBackward0>)\n",
            "400 Loss:  tensor([6.6771], grad_fn=<SubBackward0>)\n",
            "10 Loss:  tensor([103.8806], grad_fn=<SubBackward0>)\n",
            "20 Loss:  tensor([30.3536], grad_fn=<SubBackward0>)\n",
            "30 Loss:  tensor([128.3210], grad_fn=<SubBackward0>)\n",
            "40 Loss:  tensor([87.5446], grad_fn=<SubBackward0>)\n",
            "50 Loss:  tensor([70.9396], grad_fn=<SubBackward0>)\n",
            "60 Loss:  tensor([75.6532], grad_fn=<SubBackward0>)\n",
            "70 Loss:  tensor([61.0656], grad_fn=<SubBackward0>)\n",
            "80 Loss:  tensor([95.2310], grad_fn=<SubBackward0>)\n",
            "90 Loss:  tensor([103.9698], grad_fn=<SubBackward0>)\n",
            "100 Loss:  tensor([67.2094], grad_fn=<SubBackward0>)\n",
            "110 Loss:  tensor([59.0300], grad_fn=<SubBackward0>)\n",
            "120 Loss:  tensor([93.5100], grad_fn=<SubBackward0>)\n",
            "130 Loss:  tensor([87.0298], grad_fn=<SubBackward0>)\n",
            "140 Loss:  tensor([54.8864], grad_fn=<SubBackward0>)\n",
            "150 Loss:  tensor([37.3715], grad_fn=<SubBackward0>)\n",
            "160 Loss:  tensor([80.5049], grad_fn=<SubBackward0>)\n",
            "170 Loss:  tensor([9.6737], grad_fn=<SubBackward0>)\n",
            "180 Loss:  tensor([101.5703], grad_fn=<SubBackward0>)\n",
            "190 Loss:  tensor([14.8766], grad_fn=<SubBackward0>)\n",
            "200 Loss:  tensor([83.9872], grad_fn=<SubBackward0>)\n",
            "210 Loss:  tensor([101.5663], grad_fn=<SubBackward0>)\n",
            "220 Loss:  tensor([106.5408], grad_fn=<SubBackward0>)\n",
            "230 Loss:  tensor([96.1567], grad_fn=<SubBackward0>)\n",
            "240 Loss:  tensor([89.9221], grad_fn=<SubBackward0>)\n",
            "250 Loss:  tensor([84.4788], grad_fn=<SubBackward0>)\n",
            "260 Loss:  tensor([8.4775], grad_fn=<SubBackward0>)\n",
            "270 Loss:  tensor([63.2095], grad_fn=<SubBackward0>)\n",
            "280 Loss:  tensor([54.2986], grad_fn=<SubBackward0>)\n",
            "290 Loss:  tensor([38.2852], grad_fn=<SubBackward0>)\n",
            "300 Loss:  tensor([99.8143], grad_fn=<SubBackward0>)\n",
            "310 Loss:  tensor([132.8068], grad_fn=<SubBackward0>)\n",
            "320 Loss:  tensor([88.1460], grad_fn=<SubBackward0>)\n",
            "330 Loss:  tensor([5.7456], grad_fn=<SubBackward0>)\n",
            "340 Loss:  tensor([78.1704], grad_fn=<SubBackward0>)\n",
            "350 Loss:  tensor([64.1833], grad_fn=<SubBackward0>)\n",
            "360 Loss:  tensor([129.4737], grad_fn=<SubBackward0>)\n",
            "370 Loss:  tensor([95.7809], grad_fn=<SubBackward0>)\n",
            "380 Loss:  tensor([112.7996], grad_fn=<SubBackward0>)\n",
            "390 Loss:  tensor([95.1412], grad_fn=<SubBackward0>)\n",
            "400 Loss:  tensor([6.5742], grad_fn=<SubBackward0>)\n",
            "Training finished!!!!!:\n",
            "(tensor(1124.0372), [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 6, 6, 1, 1, 1, 1])\n",
            "tensor([[ 1203,  2303,    38,  ...,   106, 43184,    24],\n",
            "        [41353,  1709,    42,  ...,  1397,     9,  5695],\n",
            "        [10740,  1370,   195,  ...,    21,  4795,    23],\n",
            "        ...,\n",
            "        [   50,   201,   270,  ...,    55,   356,   757],\n",
            "        [24168, 15476,   457,  ...,     0,  5888,    38],\n",
            "        [11087,    51,    62,  ...,  2634,  3179,   551]])\n",
            "Predic type: <class 'list'>\n",
            "['GENE_OR_GENOME' 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O'\n",
            " 'DISEASE_OR_SYNDROME' 'DISEASE_OR_SYNDROME' 'DISEASE_OR_SYNDROME'\n",
            " 'DISEASE_OR_SYNDROME' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'O' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'DISEASE_OR_SYNDROME' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'DISEASE_OR_SYNDROME' 'GROUP' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'DISEASE_OR_SYNDROME' 'O' 'O' 'O' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'CELL' 'O' 'O' 'DISEASE_OR_SYNDROME'\n",
            " 'DISEASE_OR_SYNDROME' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O' 'O' 'O' 'O' 'DISEASE_OR_SYNDROME'\n",
            " 'GROUP' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'CHEMICAL' 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y0WyS08fx8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65dc2f37-7a3a-452f-9991-db311351dca8"
      },
      "source": [
        "with torch.no_grad():\n",
        "  print(model(torch.tensor(val_tokens[5],dtype=torch.long))[1])                 \n",
        "  print(model)\n",
        "print(val_preds[2]) \n",
        "\n",
        "print(len(val_preds))\n",
        "print(val_preds.size)\n",
        "\n",
        "print(len(val_dict[\"id\"])) \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 3, 1, 1, 1, 6, 1, 1, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 13, 1, 1, 22, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1]\n",
            "BiLSTM_CRF(\n",
            "  (word_embeds): Embedding(82275, 32)\n",
            "  (lstm): LSTM(32, 16, bidirectional=True)\n",
            "  (hidden2tag): Linear(in_features=32, out_features=67, bias=True)\n",
            ")\n",
            "['GENE_OR_GENOME' 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O'\n",
            " 'DISEASE_OR_SYNDROME' 'DISEASE_OR_SYNDROME' 'DISEASE_OR_SYNDROME'\n",
            " 'DISEASE_OR_SYNDROME' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'O' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'DISEASE_OR_SYNDROME' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'DISEASE_OR_SYNDROME' 'GROUP' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'DISEASE_OR_SYNDROME' 'O' 'O' 'O' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'CELL' 'O' 'O' 'DISEASE_OR_SYNDROME'\n",
            " 'DISEASE_OR_SYNDROME' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O' 'O' 'O' 'O' 'DISEASE_OR_SYNDROME'\n",
            " 'GROUP' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'CHEMICAL' 'O']\n",
            "2950\n",
            "377600\n",
            "2950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBYrWyhG_B7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2763b4-55ed-4750-8b96-14057ea30f9e"
      },
      "source": [
        "\n",
        "#f_dict\n",
        "def get_pred_csv(pred, f_dict, padding_id=\"_w_pad_\"):\n",
        "    ids = [str(i)+\"_\"+str(w_id) for i, _ in enumerate(f_dict[\"id\"]) for w_id, word in enumerate(f_dict[\"word_seq\"][i]) if word != padding_id]\n",
        "    tags = [ps[w_id] for i, ps in enumerate(pred) for w_id, word in enumerate(f_dict[\"word_seq\"][i]) if word != padding_id]\n",
        "    return {\"id\":ids, \"tag\":tags}\n",
        "#accuracy(D1,D2)\n",
        "def get_csv_accuracy(d1, d2):\n",
        "    assert len(d1[\"id\"]) == len(d2[\"id\"])\n",
        "    return sum(np.array(d1[\"tag\"]) == np.array(d2[\"tag\"])) / len(d1[\"tag\"])\n",
        "    \n",
        "val_pred_dict = get_pred_csv(val_preds, val_dict)\n",
        "# Calculate acuracy rate\n",
        "print(len(val_pred_dict[\"id\"])) \n",
        "print(len(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Kaggle/Kaggle_input/val_labels.csv\")))\n",
        "print(\"validation acc\", get_csv_accuracy(val_pred_dict, pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Kaggle/Kaggle_input/val_labels.csv\")))\n",
        "pd.DataFrame(val_pred_dict).to_csv(\"val_pred_final.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "345690\n",
            "345690\n",
            "validation acc 0.7696780352338801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROTjbhXGVNBK",
        "outputId": "40a316a3-f34f-47a3-84a5-0723901c77b2"
      },
      "source": [
        "predict=[]\n",
        "Counter=0\n",
        "with torch.no_grad():\n",
        "  print(model(torch.tensor(test_tokens[1],dtype=torch.long)))\n",
        "with torch.no_grad():\n",
        "  print(torch.tensor(test_tokens,dtype=torch.long))\n",
        "  for test_token in test_tokens:\n",
        "    predict.append(model(torch.tensor(test_token,dtype=torch.long))[1])  \n",
        "    \n",
        "\n",
        "print(\"Predic type:\", type(predict))\n",
        "\n",
        "test_preds = np.array([[idx2tag[p] for p in pred_tor] for pred_tor in predict])\n",
        "print(test_preds[2]) \n",
        "test_preds_dict = get_pred_csv(test_preds, test_dict)\n",
        "print(len(test_preds_dict[\"id\"])) \n",
        "pd.DataFrame(test_preds_dict).to_csv(\"test_pred_final.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor(1487.7362), [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([[    0,  2372,  1144,  ...,   358,     3,     0],\n",
            "        [   50,  3185,     3,  ...,     1,     1,     1],\n",
            "        [17744,   561,   106,  ...,    55, 10061,   106],\n",
            "        ...,\n",
            "        [ 4639,     3, 21515,  ...,  6100,     3,  1218],\n",
            "        [ 4580, 11637,     0,  ...,  1836,    96,  6890],\n",
            "        [37233,   106,  6983,  ...,    51, 33627,    61]])\n",
            "Predic type: <class 'list'>\n",
            "['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O' 'O'\n",
            " 'DISEASE_OR_SYNDROME' 'DISEASE_OR_SYNDROME' 'DISEASE_OR_SYNDROME' 'O'\n",
            " 'GENE_OR_GENOME' 'O' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME'\n",
            " 'GENE_OR_GENOME' 'O' 'ORGANISM' 'ORGANISM' 'ORGANISM' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'GENE_OR_GENOME' 'GENE_OR_GENOME' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O' 'O' 'CHEMICAL' 'CHEMICAL' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O']\n",
            "349105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmk7dmn1u1Z7",
        "outputId": "de353d25-d1f6-4b21-d91f-966e2c08fcec"
      },
      "source": [
        "print(len(test_preds_dict[\"id\"])) \n",
        "pd.DataFrame(test_preds_dict).to_csv(\"submission2.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "349105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsdEuS6rrly1"
      },
      "source": [
        "torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Kaggle/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lBU3xXZr7HD"
      },
      "source": [
        "https://www.cnblogs.com/xiximayou/p/12450761.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXKBfWhVmtWv"
      },
      "source": [
        "state = { 'model': model.state_dict(),'optimizer':optimizer.state_dict(),'epoch':EPOCH}   \n",
        "torch.save(state,'/content/drive/MyDrive/Colab Notebooks/Kaggle/model2.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oHJnHslWaty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229215fb-0625-44a9-fff5-df26b9ecae20"
      },
      "source": [
        "\n",
        "test_preds_dict = get_pred_csv(test_preds, test_dict)\n",
        "print(len(test_preds_dict[\"id\"])) \n",
        "pd.DataFrame(test_preds_dict).to_csv(\"test_predfinal.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2791221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU3tjst_W1WX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "3bdb1534-a29d-42a3-be8d-e137fef9b4e4"
      },
      "source": [
        "# Please submit this file\n",
        "pd.read_csv(\"train_predfinal.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_1</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_2</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_3</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_4</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791216</th>\n",
              "      <td>23599_123</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791217</th>\n",
              "      <td>23599_124</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791218</th>\n",
              "      <td>23599_125</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791219</th>\n",
              "      <td>23599_126</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2791220</th>\n",
              "      <td>23599_127</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2791221 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                id tag\n",
              "0              0_0   O\n",
              "1              0_1   O\n",
              "2              0_2   O\n",
              "3              0_3   O\n",
              "4              0_4   O\n",
              "...            ...  ..\n",
              "2791216  23599_123   O\n",
              "2791217  23599_124   O\n",
              "2791218  23599_125   O\n",
              "2791219  23599_126   O\n",
              "2791220  23599_127   O\n",
              "\n",
              "[2791221 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPkWZ4i4h3av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "d5f379de-77bc-4c4d-b195-cfc3292e4d06"
      },
      "source": [
        "# Please submit this file\n",
        "pd.read_csv(\"val_pred.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_1</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_2</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_3</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_4</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345685</th>\n",
              "      <td>2949_123</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345686</th>\n",
              "      <td>2949_124</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345687</th>\n",
              "      <td>2949_125</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345688</th>\n",
              "      <td>2949_126</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345689</th>\n",
              "      <td>2949_127</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>345690 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id tag\n",
              "0            0_0   O\n",
              "1            0_1   O\n",
              "2            0_2   O\n",
              "3            0_3   O\n",
              "4            0_4   O\n",
              "...          ...  ..\n",
              "345685  2949_123   O\n",
              "345686  2949_124   O\n",
              "345687  2949_125   O\n",
              "345688  2949_126   O\n",
              "345689  2949_127   O\n",
              "\n",
              "[345690 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}